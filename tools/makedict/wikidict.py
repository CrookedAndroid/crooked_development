#!/usr/bin/python

'''
	wikidict -- build dictionary from word counts
	Copyright (C) 2011 The Android Open Source Project

	Licensed under the Apache License, Version 2.0 (the "License");
	you may not use this file except in compliance with the License.
	You may obtain a copy of the License at

		http://www.apache.org/licenses/LICENSE-2.0

	Unless required by applicable law or agreed to in writing, software
	distributed under the License is distributed on an "AS IS" BASIS,
	WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
	See the License for the specific language governing permissions and
	limitations under the License.
'''

# usage: ./wikicount.py *.json > dict.xml

# TODO: this is extremely memory intensive
# took 20 min and 7GB ram to chew through summarized corpus

import re, sys, simplejson, math

ARTICLE, TOTAL = range(2)

class Word:
	def __init__(self):
		self.variety = {}

	def report(self, word, counts):
		if word not in self.variety:
			self.variety[word] = counts
		else:
			self.variety[word][ARTICLE] += counts[ARTICLE]
			self.variety[word][TOTAL] += counts[TOTAL]

	def score(self):
		# calcuate popularity score for this word
		# weighted more towards number of articles instead of total frequency
		# should favor words which are used widely, not just spammed
		combined = [0,0]
		for word, counts in self.variety.iteritems():
			combined[ARTICLE] += counts[ARTICLE]
			combined[TOTAL] += counts[TOTAL]
		return (0.7*math.log(combined[ARTICLE])) + (0.3*math.log(combined[TOTAL]))

	def best(self):
		# find best representation of this word
		best = [ (word, counts[TOTAL]) for word, counts in self.variety.iteritems() ]
		best.sort(key=lambda row: -row[1])
		if len(best) > 1:
			topword = best[0][0]
			ratio = best[0][1] / best[1][1]
			if ratio > 3:
				# when 3x more frequent, pick this variant
				return topword
			else:
				# otherwise lowercase
				return topword.lower()
		else:
			return best[0][0]



words = {}

for infilename in sys.argv[1:]:
	print >> sys.stderr, "loading %s..." % (infilename),
	with open(infilename) as infile:
		raw_words = simplejson.load(infile)

	print >> sys.stderr, "reporting...",
	for word, counts in raw_words.iteritems():
		key = word.lower()
		if key not in words:
			words[key] = Word()
		words[key].report(word, counts)

	print >> sys.stderr, "total", len(words), "words"


# ignore some words that might have snuck through

ignore_single = ["b","c","d","e","f","g","h","j","k","l","m","n","o","p","q","r","s","t","u","v","w","x","y","z"]
ignore_random = ["th","px","pp","ed","ii","iii","iv","nd","en","ch","id","da","du","di","il","rd","gt","lt"]

ignore_profane = [
	re.compile("sh%st" % ("i")),
	re.compile("f%sck" % ("u")),
	re.compile("^p%sss" % ("i")),
	re.compile("^c%snt$" % ("u")),
	re.compile("^c%scks%sck" % ("o","u")),
	re.compile("^f%sg" % ("a")),
	re.compile("^n%sgg" % ("i")),
]

found_profane = []
for word in words.keys():
	for re_profane in ignore_profane:
		if re_profane.search(word):
			found_profane.append(word)

def ignore_words(ignore):
	for word in ignore:
		if word in words:
			del words[word]

ignore_words(ignore_single)
ignore_words(ignore_random)
ignore_words(found_profane)


# finally sort and filter to top ~70k
print >> sys.stderr, "sorting...",
words = [ (word.best(), word.score()) for word in words.itervalues() ]
words.sort(key=lambda row: -row[1])
words = words[:75000]

score_high = words[0][1]
score_low = words[-1][1]

print "<!-- generated by wikidict.py --><wordlist>"
for word, score in words:
	score = (score - score_low) / (score_high - score_low)
	score *= 254
	print '<w f="%d">%s</w>' % (score+1, word)
print "</wordlist>"

print >> sys.stderr, "done"
